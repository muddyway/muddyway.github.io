---
layout: post
title: '端到端'
date: 2019-09-14
author: muddyway
cover: ''
tags: network
---
# END-TO-END ARGUMENTS IN SYSTEM DESIGN
[pdf](http://219.219.220.50:443/UploadFiles/schoolWorks/20190913/Advanced_Network2019@SSE_Paper_reading01_endtoend_20199132342174.pdf)  
   分布式系统各模块间功能设计,底层实现较多内置功能是实现困难、网络代价高、不必要的。当时还没有清晰原则指导系统设计者，作者论文很好地帮助他们使用考虑**端到端论点**解决通信传输**可靠性问题**。  
   为了证明**可靠传输的必要性**，作者举了有趣的例子：MIT没有意识到网络数据不被保护被迫尝试人工纠错。作者举例文件传输过程及对可靠传输存在的威胁，还有解决这些威胁的方法。定位错误、故障回复难以实现的情况下，使用**超时重传、选择冗余、序列号确认**是可行的，这有点像TCP。   
   那为什么用**底层保证可靠性不行**呢？
   提高底层网络可靠性来防止人工纠错，这影响性能，尤其是当年交换机等带宽很有限(就像当年写汇编还需要用绝对地址换入换出)。悲观的是，保证底层可靠需要更多代价，这基于两点：1.**底层是公共的，应用未必都需要**；2.**底层有效信息少，操作起来效率低**。  
   每个应用根据自身需要实现可靠传输是端到端论点支持的，虽然这可能使总成本比在底层传输还要高(这可能是后来单独设置tcp层的原因？)，但这使应用程序有了选择余地也使他们有更多信息。  
 那除了以上两点，在**邮件系统、数据加密、数据库磁盘事务控制、类似分组交换的应用**中底层保证可不可行呢，作者根据自己经验和事实网络协议发展历史给出了自己看法：不行。  
 关于端到端传输其他一些例子：**邮件传输**，**没有必要返回给发件人每条消息的确认**，发件人只关心收件人最终是否做到。另外在处理足够复杂数据需要**对每条消息分阶段确认**时，在高层使用可靠性保证是明智的。    
 另一个领域是**数据加密**，端到端可以**被信任地安全地管理所需密钥**，并对于非透明传输能进行主体身份识别，从而保证传输数据不会被公开。另外，如果想通过**防火墙**一劳永逸解决底层传输问题是不行的，但对于验证加密后特定用户特定数据的真实性，这种机制便很难行得通。      
另外在**重复消息抑制**方面，**检测、处理两次相同发送消息很麻烦**，而应用程序级别无论如何都**必须具有重复抑制机制**，比如多站点事务的一端处理需要通过延迟确认验证系统崩溃。  
在基于**FIFO的类似分组交换网络**中，通信子系统必须保证时序来组装分组包。对于数据库或磁盘读写的复杂事务管理，在读取返回时，如果不能消除多余传递确认，那对磁盘访问负载量将产生显著影响。**语音**适当延迟和丢包(dropout)是被接受的，端到端在分组排序和重复抑制做得很好，但并不意味绝对要做，他是**灵活**的，这有点像tcp和udp结合。  
**没必要消除端到端传输的罕见错误**，因为代价太高。即使在银行交易(需要有审计报告)和机票预定实时系统(表现为延迟订不到票)中。  
在80年代作者就有**分层概念、有数据传输可靠性**保证详细思考，虽然没有公式推导和太多引用，但根据多年经验和一些事实(如mit工程师事件)和朋友的帮助提出了**底层传输不可行**，并从多方（邮件系统、数据加密、数据库磁盘事务控制、类似分组交换的应用）提出否定，并提出端到端也不是一成不变，而是在如**语音传输中不需要可靠性保证**，也为tcp udp雏形提供依据，虽然现在华为每台三层交换机运行高级语言代码超过几万行，并通过交换机能直接防止洪泛攻击等与作者**底层精简观点**不符合，但也是磁盘存储高速发展的结果，不管怎么样，这篇论文都是跨时代意义的经典。  

In the design of functions among modules of distributed system, it is difficult to realize many built-in functions at the bottom, and the cost of network is high and unnecessary. At that time, there was no clear principle to guide system designers. The author's paper helped them solve the reliability problem of communication transmission by considering the end-to-end argument.

The author gives examples of file transfer process and existing threats, as well as solutions to these threats for reliable transmission. It is feasible to use timeout retransmit, select redundancy and confirm sequence number when locating redundancy and fault recovery are difficult to achieve, which is a bit like TCP. In order to prove the necessity of reliable transmission, the author also gives an interesting example: MIT is not aware that network data is not protected and forced to attempt manual error correction.

So why is it not possible to guarantee reliability with the bottom?

Improve the reliability of the underlying network to prevent manual error correction, which affects performance, especially when the bandwidth of switches is very limited (like writing assembly in that year also need to use absolute address swap in and out). Pessimistic is that guaranteeing the reliability of the underlying layer requires more cost, which is based on two points: 1. the underlying layer is public, and the application may not be required; 2. the underlying layer has less effective information and is inefficient to operate.

The end-to-end argument supports reliable transmission for each application based on its own needs, although this may result in higher total cost than transmission at the bottom level (which may be the reason why the TCP layer was later set up separately? But it gives the application a choice and gives them more information.

In addition to the above two points, it is not feasible to guarantee the underlying level in mail system, data encryption, Database disk transaction control, and similar packet switching applications. The author gives his own views based on his own experience and the history of network protocol development: No.

There are other examples of end-to-end transmission: there is no need to return the confirmation of each message to the sender, who only cares if the recipient finally does it. In addition, it is advisable to use reliability assurance at high levels when processing sufficiently complex data requires phased validation of each message.

Another area is data encryption, end-to-end can be trusted to manage the required key safely, and for opaque transmission can be subject identification, so as to ensure that the transmission data will not be disclosed. In addition, if you want to solve the underlying transmission problem once and for all through the firewall, it is not feasible, but for verifying the authenticity of the encrypted specific user data, this mechanism will be difficult to work.

In addition, in the aspect of duplicate message suppression, it is very difficult to detect and process the same message twice, and application level must have duplicate suppression mechanism in any case, such as one end of multi-site transaction processing needs to verify system crash by delayed confirmation.

In FIFO-based similar packet switching networks, the communication subsystem must guarantee the timing to assemble the packet. For the complex transaction management of database or disk read-write, if the redundant transfer confirmation can not be eliminated when reading and returning, it will have a significant impact on the disk access load. Appropriate voice delay and dropout are acceptable. End-to-end sorting and duplication suppression are well done, but it does not mean absolute need to be done. It is flexible, which is a bit like the combination of TCP and udp.

There is no need to eliminate rare errors in end-to-end transmission, because the cost is too high. Even in bank transactions (requiring audit reports) and real-time ticket booking systems (manifested as delays in booking tickets).

In the 1980s, the author had a detailed thinking about the concept of layered data transmission and reliability assurance. Although there was no formula deduction and too many references, based on years of experience and some facts (such as MIT Engineer events) and the help of friends, the author proposed that the underlying transmission was not feasible, and from many aspects (mail system, data encryption, Database disk affairs). Business control, similar packet switching applications) put forward a negative, and put forward that end-to-end is not unchanged, but in voice transmission does not need reliability assurance, but also provides a basis for the prototype of TCP udp, although Huawei three-tier switches now run more than tens of thousands of lines of high-level language code, and through the switch directly defend. Flood control and flooding attacks are not in line with the author's view of bottom streamlining, but they are also the result of the rapid development of disk storage. Anyway, this paper is a cross-era classic.
 
 
